---
title: "Introduction to Classification"
output: html_notebook
---

```{r,warning=FALSE,message=FALSE}
library(tidyverse)
library(ISLR)
library(broom)
library(caret)
library(ROCR)
```


# Introduction 

Recall the iris data set:
```{r}
head(iris)
```

Attempting to predict the species of a flower based on the length and width measurements is a **classification** problem. In this case, the reponse variable assumes one of three possible class values: setosa, virginica, or versicolor. Let's look at the reponse variable:
```{r}
table(iris$Species)
```

This is a multi-class classification problem since there are more than two distinct classes in wich observations may fall. 

There is a special type of classification problem, **binary classification** where the categorical response variable takes values from just two distinct possible outcomes. There is a standard regression method for binary classification called logistic regression. Since regression is already a familiar concept, we will begin by discussing logistic regression, how it is implemented in R, and how to apply it. 

# Logistic Regression

Consider the Default data set from the ISLR package:
```{r}
?Default
```


```{r}
dim(Default)
```



```{r}
head(Default)
```

```{r}
table(Default$default)
```

Let's consider a simple problem. Can we use the average balance that the customer has remaining on their credit card after making their monthly payment (balance variable) to predict if a customer will default on their loan or not? 


Let's start with some visualizations:
```{r}
Default %>% ggplot(aes(x=balance,color=default)) + geom_freqpoly()
```


```{r}
Default %>% ggplot(aes(x=balance,y=balance,color=default)) + 
 geom_point()
```

**Question:** What do these plots tell us? 

We will make one more plot that may be helpful by first converting the default variable into a numerical variable that takes on either 0 (for No) or 1 (for Yes). 
```{r}
Default_to_fit <- Default %>% mutate(default_num=ifelse(default == "Yes",1,0))
```

Consider the following plot:
```{r}
Default_to_fit %>% ggplot(aes(x=balance,y=default_num,color=default)) + 
 geom_point()
```

**Question:** What does this plot show? 

We will see how logistic regression can be used to model this problem. We start with the R implementation and then explain the mathematical details afterward. The basic idea is that rather than modeling the response $Y$ directly, logistic regression models the probability that $Y$ belongs to a particular category. 


We will start by creating a training and test set:
```{r}
train_ids <- createDataPartition(Default_to_fit$default_num,p=0.75,list=F)

Default_train <- Default_to_fit[train_ids, ]

Default_test <- Default_to_fit[-train_ids, ]
```

Let's look at the respective sizes of the two sets:
```{r}
(dim(Default_train))
(dim(Default_test))
```

The R implementation of logistic regression is done by the function glm() (generalized linear model) with the family arugment set to binomial. We will explain the reasons for this later. 
```{r}
default_glm_mod = train(
 form = default ~ balance,
 data = Default_train,
 trControl = trainControl(method = "repeatedcv", number = 10,repeats = 5),
 method = "glm",
 family = "binomial"
)
```

We can examine some of the resulting output:

An assessment of the performance is shown with:
```{r}
default_glm_mod$results
```

and the best model is:
```{r}
default_glm_mod$finalModel
```

## Interpreting the results from logistic regression

Logistic regression models the conditional probability $P(Y=1 | X)$ (the probability that the response is 1 given a particular value for the predictor) with the function

$P(Y=1 | X) = \frac{e^{mX + b}}{1 + e^{mX + b}}$. 

There are two points to take away from this formula

1) The right hand side has the form $f(mX + b)$, that is, the right hand side is a function of the linear expression $mX + b$.

2) The right hand side is always a number that falls between 0 and 1. For the sake of simplicity, let's use the notational convention $p(X) = P(Y=1 | X)$ to rewrite the above expression as

$p(X) = \frac{e^{mX + b}}{1 + e^{mX + b}}$, 

then some algebra shows that 

$\frac{p(X)}{1 - p(X)} = e^{mX + b}$,

which leads to 

$\ln\left( \frac{p(X)}{1 - p(X)} \right) = mX + b$. 

**Important point:** Logistic regression estimates the parameters $m$ and $b$.  Then the estimates $\hat{m}$ and $\hat{b}$ for $m$ and $b$ can be used in the formula

$p(X) = \frac{e^{mX + b}}{1 + e^{mX + b}}$

to compute the probability that $Y=1$ for a given value of the predictor $X$.  

Let's see this in our example problem:

```{r}
b_hat <- coefficients(default_glm_mod$finalModel)[1]
m_hat <- coefficients(default_glm_mod$finalModel)[2]
my_log_reg_exp <- exp(b_hat + m_hat*Default$balance)
my_log_reg <- my_log_reg_exp/(1 + my_log_reg_exp)
Default_to_fit %>% ggplot(aes(x=balance,y=default_num,color=default)) + 
 geom_point() + geom_line(data=NULL,mapping=aes(x=Default$balance,y=my_log_reg),color="black",lwd=1)
```

This shows the regression curve defined by $y = \frac{e^{\hat{m}X + \hat{b}}}{1 + e^{\hat{m}X + \hat{b}}}$. 

The question is, how do we use the probability values output from the losgitic regression model to **classify** the output as No (0) or Yes (1). This invloves setting a **threshold** value. For example, we might say, any balance value that leads to a probability that $Y=1$ that is greater than 0.5 should classify the response as Yes (or 1). This is called the **decision criterion**. Suppose we set our decision criterion at 0.5, then the pictures looks like
```{r}
b_hat <- coefficients(default_glm_mod$finalModel)[1]
m_hat <- coefficients(default_glm_mod$finalModel)[2]
my_log_reg_exp <- exp(b_hat + m_hat*Default$balance)
my_log_reg <- my_log_reg_exp/(1 + my_log_reg_exp)
Default_to_fit %>% ggplot(aes(x=balance,y=default_num,color=default)) + 
 geom_point() + geom_line(data=NULL,mapping=aes(x=Default$balance,y=my_log_reg),color="black",lwd=1) + 
  geom_hline(yintercept = 0.5,color="red")
```

**Question:** What does this plot show? 

Obviously there is a judgment call that is being made here. Later we will look at a technique (the receiver operator characteristic (ROC)) for assessing the consequences of choosing a particular decision criterion. For now, let's look at the classification error rate obtained by using our decision criterion to make classifications. 

A typical method for displaying the results of a classification is with a **confusion matrix**
```{r}
table(Default_test$default,predict(default_glm_mod,newdata = Default_test))
```

There is also a confusion matrix function in caret:
```{r}
confusionMatrix(predict(default_glm_mod,newdata = Default_test),Default_test$default,positive = "Yes")
```


Thus, the test classification error rate is
```{r}
(59 + 9)/(2402 + 9 + 59 + 30)
```

Also observe that
```{r}
1 - 0.0272
```
is exactly the quantity output as Accuracy by the confusionMatrix() function. 

How do we know that the predict() function used the 0.5 threshold for the decision criterion for classification? It is possible to return the probability values with the predict function:
```{r}
head(predict(default_glm_mod$finalModel,newdata = Default_test,type="response"),10)
```

Let's apply our decision criterion to obtain classification values:
```{r}
pred_probs <- predict(default_glm_mod$finalModel,newdata = Default_test,type="response")
pred_classes <- ifelse(pred_probs > 0.5,"Yes","No")
table(predict(default_glm_mod,newdata = Default_test),pred_classes)
```

Let's plot the predictions and the known results together:
```{r}
Default_to_fit_test <- Default_test %>% mutate(default_num=ifelse(default == "Yes",1,0))
Default_to_fit_test$pred_class <- predict(default_glm_mod,newdata = Default_test)
Default_to_fit_test <- Default_to_fit_test %>% mutate(default_num_pred=ifelse(pred_class == "Yes",1,0))
Default_to_fit_test %>% ggplot(aes(x=balance)) + 
  geom_point(aes(y=default_num,color=default)) + 
  geom_point(aes(y=default_num_pred,shape=pred_class))
```

Compare this plot with the confuion matrix. 


Evaluating the results of a classification model means measuring how accurately our predicted labels match the "gold standard" labels in the evaluation set. For the common case of
two distinct labels or classes (binary classification), we sometimes call the more interesting of the two classes (*e.g.* Yes (1) in the Default data) as positive and the other class as
negative (*e.g.* No (0) in the Default data). Notice that we specified the positive class in the confusionMatrix() function. 

The confusion matrix is very helpful in evaluating classifications and there are several quantities that can be derived from the confusion matrix.   

1) *True Positives (TP)*: Here our classifier labels a positive item as positive, resulting in a win for the classifier.
2) *True Negatives (TN)*: Here the classifier correctly determines that a member of the negative class deserves a negative label. Another win.
3) *False Positives (FP)*: The classifier mistakenly calls a negative item as a positive, resulting in a “type I” classification error.
4) *False Negatives (FN)*:  The classifier mistakenly declares a positive item as negative, resulting in a “type II” classification error.

The the accuracy of a classifier is given as

$\text{Accuracy} = \frac{TP + TN}{TP + TN + FP + FN}$. 

**Note:** Sometimes you see the following notation:

* $N_{+} = TP + FN$ - the number of true positives
* $N_{-} = TN + FP$ - the number of true negatives
* $\hat{N}_{+}=TP + FP$ - the number of classified positives
* $\hat{N}_{-}=TN + FN$ - the number of classified negatives

Then the **true positive rate** (TPR) is 

$TPR = \frac{TP}{N_{+}} = \frac{TP}{TP + FN}$,

and 

the **false positive rate** (FPR) is 

$FPR = \frac{FP}{N_{-}} = \frac{FP}{TN + FP}$. 


A **reciever operator characteristic (ROC) curve** proivdes a way to visualize the performance of a binary classifier as the threshold (decision criterion) varies from 0 to 1. 

## ROC Curves

An ROC curve plots the **true positive rate** versus the **false positive rate**.  There are two important questions that an ROC curve helps to address:

1) What is the overall quality of classifier? The area under the ROC curve (AUC) provides a quantitative measure of this. 
2) What is an effective theshold (decision criterion)? The ROC curve provides a visual representation of our complete space of options in putting together a classifier. So an ROC curve will help us to pick the best option.

There are several functions in the ROCR package that enable us to construct and ROC curve and to compute the AUC. 

```{r}
temp_pred <- prediction(predict(default_glm_mod$finalModel,Default_test),Default_test$default)
temp_perf <- performance(temp_pred,"tpr","fpr")
plot(temp_perf)
```

To do this with ggplot we need to extract more information. One way to do this is as follows:
```{r}
temp_rates <- predict(default_glm_mod$finalModel,Default_test,type = "response") %>%
 prediction(Default_test$default) %>% performance("tpr","fpr")
fpr <- temp_rates@x.values[[1]]
tpr <- temp_rates@y.values[[1]]
threshold_vals <- temp_rates@alpha.values[[1]]
ROC_df <- data.frame(fpr=fpr,tpr=tpr,threshold_vals=threshold_vals)
```

```{r}
ROC_df %>% ggplot(aes(x=fpr,y=tpr)) + geom_line()
```

Consider what happens as we increase our threshold from 0 to 1. 

```{r}
ROC_df %>% ggplot(aes(x=fpr,y=tpr,color=threshold_vals)) + 
  geom_line() + 
  scale_colour_gradientn(colours = terrain.colors(10))
```


Every time we change our threshold value, we either increase
the number of true positives (if this example was positive) or false positives (if this example was in fact a negative). At the very left, we achieve true/false
positive rates of 0%, since the classifier labeled nothing as positive at that cutoff.
Moving as far to the right as possible, all examples will be labeled positively, and
hence both rates become 100%. Each threshold in between defines a possible
classifier, and the sweep defines a staircase curve in true/false positive rate space
taking us from (0%,0%) to (100%,100%).

This also explains why the area under the curve is a quantitative measure the effectiveness of a classification model. 

This command computes the area under the ROC curve:
```{r}
predict(default_glm_mod$finalModel,Default_test,type = "response") %>%
 prediction(Default_test$default) %>% performance("auc") %>% attr("y.values")
```

**Note** The maximum possibel AUC is 1, the closer to 1, the more effective the classifier. 


# KNN for Classification

This [online demo](https://codepen.io/gangtao/pen/PPoqMW) is very helful. 
