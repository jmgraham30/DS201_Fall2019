---
title: "Introduction to Data Modeling"
output: html_notebook
---

```{r,message=FALSE,warning=FALSE}
library(tidyverse)
library(GGally)
library(caret)
library(broom)
library(ISLR)
```


# Introduction 

So far we have studied the following steps in the data science workflow:

1) Obtain data
2) Import data 
3) Exploratory data analysis: clean, wrangle, summarize, visualize  
4) Determine questions to ask

The next step is to use modeling in order to answer the questions that arise from an EDA. In a moment we will explain what we mean by modeling. For now, note that what we will discuss has significant relation with traditional probability and statistcs, machine leanring, statistical learning, predictive analytics, artificial intelligence, pattern recognition, data mining, etc. In all of these topics a central goal is to derive insight or understanding from data.  For simplicity, we will refer to any of an assortment of related ideas that seek to derive insight or understanding from data as modeling or data modeling. 

Some of the most useful references for the material we cover on modeling are

1) [An Introduction to Statistical Learning](http://faculty.marshall.usc.edu/gareth-james/ISL/) (Note that there are also video lectures that accompany this book.) 
2) [Elements of Statistical Learning](https://web.stanford.edu/~hastie/ElemStatLearn/)
3) [Mathematics for Machine Leanring](https://mml-book.github.io/)
4) [The Hundred-Page Machine Leanring Book](http://themlbook.com/) 
5) [An Introduction to Machine Learning with R](http://bit.ly/intromlr)

We will use large parts of the first book as it provides a nice blend of theory and practice. It is also widely regarded as an excellent introduction to statistical learning. The second book is a canonical reference for the theory of statistical (machine) learning, this book is mathematically sophisticated. The third book is a very new and very good reference that you can use to learn most of the mathematics you need in order to understand the presentation in Elements of Statistical Learning. The fourth book provides a clear and concise yet highly readable description of many common data modeling methods. There is a [GitHub repo](https://github.com/aburkov/theMLbook) that contains python code that illustrates the implementation of the methods described in the book. The fifth book provides a convenient (and free) reference for the R implementation of many common data modeling algorithms.   


We will proceed in our study of modeling as follows:

1) We begin by considering several motivating examples. For this please read Chapter 1 from [An Introduction to Statistical Learning](http://faculty.marshall.usc.edu/gareth-james/ISL/)
2) Next, we will describe the main "big picture" ideas, concepts, and terminology of data modeling. For this please read Chapter 2 from [An Introduction to Statistical Learning](http://faculty.marshall.usc.edu/gareth-james/ISL/)
3) Then, we will look at three specific modeling algorithms or methods in detail. Specifically, we will consider linear regression, lowess regression, and k nearest neighbors. Simultaneously, we will introduce in an intuitive manner the necessary statistical ideas that we need to work with data models. 
4) Finally, we will examine an assortment of modeling methods or algorithms. Here we will largely ignore theory and focus on dveeloping a thorough yet intuitive understanding of how to use these methods in practice. 

# Some Motivation 

## Wage data and predicting income

We begin by considering the Wage data set that is contained in the ISLR package. This package contains data and code that accompanies the book [An Introduction to Statistical Learning](http://faculty.marshall.usc.edu/gareth-james/ISL/). 

Let's determine the size of the data set and get a sense of what it contains:
```{r}
dim(Wage)
```

```{r}
head(Wage)
```

```{r}
?Wage
```

The primary question of interest is "what are the factors that influence ones income?" In this context then the wage variable is our outcome or response variable and the other variables are predictors. Things can become complicated very quickly so we will being with a simpler but related question. What is the impact of ones age (quantitative), level of education (categorical), and time (quantitative) on wages? 

Let's begin with some visualizations:
```{r}
Wage %>% ggplot(aes(x=age,y=wage)) + geom_point()
```

**Question:** What do you observe from this data?

```{r}
Wage %>% ggplot(aes(x=age,y=wage)) + geom_point() + geom_smooth()
```

**Question:** How does adding the smoothing curve help or hinder in seeing a pattern? 

```{r}
Wage %>% ggplot(aes(x=year,y=wage)) + geom_point() + geom_smooth(method = "lm")
```

**Question:** What do you observe about the data from the above graph? 

```{r}
Wage %>% mutate(ed_level_number=str_extract(education,"\\d")) %>% select(education,ed_level_number,wage) %>%
  ggplot(aes(x=ed_level_number,y=wage,color=ed_level_number)) + geom_boxplot()
```

**Question:** What do you observe about the data from the above graph? 

**Question:** Do you think we can use age, year, or education to predict an individuals wage? 

## Iris data and predicting flower species

The [Iris data set](https://en.wikipedia.org/wiki/Iris_flower_data_set) is a famous multivariate data set introduced by the British statistician and biologist Ronald Fisher in his 1936 paper *The use of multiple measurements in taxonomic problems.* This data set is included in base R. 

Let's determine the size of the data set and get a sense of what it contains:
```{r}
dim(iris)
```

```{r}
head(iris)
```

```{r}
?iris
```

**Exercise:** By reading the iris data help page and skimming the Wikipedia article  [Iris data set](https://en.wikipedia.org/wiki/Iris_flower_data_set) state what is contained in this data set. 

The question we want to answer is can certain physical measurements, *i.e.* the lengths and widths of the iris flower sepal and petal b eused to classify the species of the plant? 

The Species (categorical) variable is the response or outcome variable while the four length measurements are the predictor (quantitative) variables. As usual it is apropriate to begin with some visualizations:
```{r}
ggpairs(iris)
```

**Question:** What do you observe about the data from this figure? 

It is helpful to focus in on some of these plots individually. 

Let's start by looking at each of the four physical measurements seperated by species
```{r}
iris %>% ggplot(aes(x=Sepal.Length,color=Species)) + geom_freqpoly()
```

We can also view this as a boxplot:
```{r}
iris %>% ggplot(aes(y=Sepal.Length,x=Species,color=Species)) + geom_boxplot()
```

**Question:** What do you observe from the last two plots? 

**Exercise:** Now you plot the other three versions. 

**Question:** Do any of the physical measurements clearly vary across species? If so, which ones? 

Here is another intereseting view of the data:
```{r}
iris %>% ggplot(aes(x=Petal.Length,y=Petal.Width,color=Species)) + geom_point()
```

**Question:** What is your observation from the last plot?

**Exercise:** What do you observe if you look at other combinations of the physical measurements with regard to how they vary across species? 


**Question:** Do you think we can use the physical measurement variables to predict the species of an iris flower? 

**Question:** What is the biggest difference in this problem compared with the problem of using  predictor variables to predict income in the Wage data? 

## Gene expression data and classifying cancers





