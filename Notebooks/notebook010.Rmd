---
title: "Multiple Linear Regression and Further Regression Topics"
output: html_notebook
---

```{r,warning=FALSE,message=FALSE}
library(tidyverse)
library(gridExtra)
library(faraway)
library(broom)
library(caret)
library(boot)
library(olsrr)
library(ISLR)
```


# Introduction 

Previously we examined simple linear regression, the situation in which we have two variables, one quantitative predictor variable $x$ and one quantitative response variable $y$. Then we assume that

$y = mx + b + \epsilon,$

and we seek to find parameter (or coefficient) values $\hat{m}$ and $\hat{b}$ such that

$\hat{y} = \hat{m}x + \hat{b},$

provides a "good" estimate for $y$.

In this notebook we extend this to the situation where there are multiple quantitative predictor variables $x_{1},x_{2},\ldots , x_{p}$ and one quantitative response variable $y$. Then we assume that

$y = m_{1}x_{1} + m_{2}x_{2} + \cdots + m_{p}x_{p} + b + \epsilon,$

and we seek to find parameter (or coefficient) values $\hat{m_{1}}, \hat{m_{2}}, \ldots , \hat{m_{p}}$ and $\hat{b}$ such that

$\hat{y} = \hat{m_{1}}x_{1} + \hat{m_{2}}x_{2} + \cdots + \hat{m_{p}}x_{p} + \hat{b},$

provides a "good" estimate for $y$.

**Note:** The estimated parameter values $\hat{m_{1}}, \hat{m_{2}}, \ldots , \hat{m_{p}}$ and $\hat{b}$ are determined by least squares in a manner similar to what we have already seen for simple linear regression. As such, we skip the details on estimation and go right to using R for multiple regression and look at applications.

# Multiple Regression in R

The lm() function works just as well for multiple regression. Let's look at an example. Consider the Boston housing data from the ISLR package:
```{r}
?MASS::Boston
Boston_df <- MASS::Boston
```

```{r}
head(Boston_df)
```

```{r}
summary(Boston_df)
```

Notice that each of these variables is quantitative although the chas variable is really a binary classification. We might be interested to try to predict the median value of a home (medv) based on the average number of rooms (rm), the  proportion of owner-occupied units built prior to 1940 (age), and the lower status of the population (lstat). 

Let's start by looking at plots of medv versus each of rm, age, and lsata individually:
```{r}
Boston_df %>% select(medv,rm,age,lstat) %>%
  pivot_longer(cols=c("rm","age","lstat")) %>% 
  ggplot(aes(x=value,y=medv)) + geom_point() + 
  facet_wrap(~name)
```
This plot is potentially misleading due to the differences in scale between these three variables, let's do one at a time:
```{r}
p1 <- Boston_df %>% ggplot(aes(x=rm,y=medv)) + geom_point()
p2 <- Boston_df %>% ggplot(aes(x=age,y=medv)) + geom_point()
p3 <- Boston_df %>% ggplot(aes(x=lstat,y=medv)) + geom_point()
grid.arrange(p1,p2,p3,nrow=1)
```

Let's compute the correlation of medv with each of rm, age, and lstat
```{r}
(with(Boston_df,cor(rm,medv)))
(with(Boston_df,cor(age,medv)))
(with(Boston_df,cor(lstat,medv)))
```

To try to predict medv using rm, age, and lstat with a linear model (*i.e.* mulitple linear regression) in R we simple use:
```{r}
Boston_lm <- lm(medv ~ rm + age + lstat,data=Boston_df)
```

Let's look at the results:
```{r}
tidy(Boston_lm)
```

**Note:** The estimate column gives us the estimated values for the coefficients. Also contained in this output is teh standard error for each estimate along with the corresponding observed t-statistic value and p-value. Even if we are not interested in inference, this information may still be useful in deciding which if any of the variables rm, age, or lstat is actually contributing to the response outcome. We will return to this discuss this in greater detail later.   

```{r}
glance(Boston_lm)
```

**Note:** This resturns statistical information related to goodness of fit.  

Any time we conduct a regression it is a good idea to plot the residuals versus the fitted values:
```{r}
augment(Boston_lm,data=Boston_df) %>% ggplot(aes(x=.fitted,y=.resid)) + geom_point()
```


**Question:** What do you observe about this plot? 

```{r}
augment(Boston_lm,data=Boston_df) %>% 
  ggplot(aes(x=.fitted,y=.resid)) + 
  geom_point() + 
  geom_hline(yintercept = 0) + 
  geom_smooth(se=FALSE)
```


**Observation:** This residual versus fitted plot suggests that the assumption of a linear relationship between medv and rm, age, lstat may be too restrictive. 


R makes it easy to fit a response to all of the predictors in the data set:
```{r}
lm(medv~.,data=Boston_df) %>% tidy()
```


We can also "update" a fit to modify what features (variables) to include:
```{r}
Boston_lm2 <- update(Boston_lm,~.-age)
```
 

```{r}
Boston_lm2 %>% tidy()
```

Let's examine the residual versus fitted plot:
```{r}
augment(Boston_lm2,data=Boston_df) %>%
  ggplot(aes(x=.fitted,y=.resid)) + 
  geom_point() + 
  geom_hline(yintercept = 0) + 
  geom_smooth(se=FALSE)
```

**Question:** There is not much of a difference between this and what we saw before, why? 

# Multiple Regression Questions

When we perform multiple linear regression, we are interested in answering one or more of the following questions:

1) Is a least one of the predictors useful in predicting the response? 

2) Do all the predictors help to explain the response, or is only a subset of the predictors useful? 

3) How well does the model fit the data? 

4) Given a set of predictor values, what response value should we predict, and how accurate is our prediction. 


## Question 1)


To answer question 1) Is a least one of the predictors useful in predicting the response? We can use a permutation test for the F-statistic to test the null hypothesis

$H_{0}$: $m_{1} = m_{2}  = \cdots = m_{p} = 0$

versus the alternatice hypothesis 

$H_{A}$: At least one of the $m_{i}$ coefficients is non-zero. 

```{r}
summary(Boston_lm)
```

Notice that we can obtain the F-statistic value from the output of lm() and do not need to use a formula for it:
```{r}
(observed_F <- summary(Boston_lm)$fstatistic[1])
```

Let's conduct our permutation test:
```{r}
N <- 5000
perm_F <- numeric(N)
for (i in 1:N){
  temp_lm <- lm(sample(medv)~rm + age + lstat,data=Boston_df)
  perm_F[i] <- summary(temp_lm)$fstatistic[1]
}
ggplot(data=NULL,mapping=aes(x=perm_F)) + 
  geom_histogram() 
```

Notice that most of the resampled F-statistic values fall around 1 whereas the observed F-statistic value is almost 300. Just intuitively we can see that our p-value will be extremely small. This the null hypothesis should be rejected. 

Observe that the output of the following command gives the p-value computed using analytic formulas:
```{r}
summary(Boston_lm)
```
The p-value for the F-statistic here is very small. 

### Another Example

Let's examine a situation where we know that there is a linear relationship between a set of predictors and a response. 

We simulate some data:
```{r}
x1 <- rnorm(45,12,3.2)
x2 <- rnorm(45,3,0.2)
x3 <- rnorm(45,27,17)
y <- 2*(x1)^2 + 3*x2*x3 - 35 + rnorm(45,sd=13)
sim_df <- data.frame(x1=x1,x2=x2,x3=x3,y=y)
```

Now let's obtain a linear regression fit and examine the results:
```{r}
lm_sim_df <- lm(y~.,data=sim_df)
```

```{r}
tidy(lm_sim_df)
```

```{r}
glance(lm_sim_df)
```
**Note:** Notice that in the data frame returned by tidy() the statistic column is the F-statistic. 

Let's plot the residual versus fitted:
```{r}
augment(lm_sim_df,data=sim_df) %>%
  ggplot(aes(x=.fitted,y=.resid)) + 
  geom_point() + geom_smooth()
```

Let's repeat our permutation test for the F-statistic to test the null hypothesis:

$H_{0}$: $m_{1}=m_{2}=m_{3} = 0$

versus the alternative

$H_{A}$: at least one of $m_{1},m_{2},m_{3}$ is not zero. 

```{r}
N <- 5000
perm_F <- numeric(N)
for (i in 1:N){
  temp_lm <- lm(sample(y)~.,data=sim_df)
  perm_F[i] <- summary(temp_lm)$fstatistic[1]
}
ggplot(data=NULL,mapping=aes(x=perm_F)) + 
  geom_histogram() 
```

Here is the observed F-statistic value:
```{r}
(observed_F <- summary(lm_sim_df)$fstatistic[1])
```

**Question:** Should we reject or fail to reject the null hypothesis? 

**Exercise:** Try this again but remove the dependence of the response $y$ in the simluted data on the preditors. 


**Exercise:** What happens if we have the response $y$ in the simluted data depend one or more of the preditors but in a non-linear fashion?


## Question 2)

Suppose that we use the F-statistic to conclude that there is sufficient evidence to reject the null hypothesis that all of the "slope" parameters in a linear regression model are zero. The next question is, exactly which of the p predictor variables are associated with the response?

This is referred to as **variable selection.** There are a few different ways to handle this. One way is by brut force where we try out all possible (linear) model combinations. There are three common approaches, the forward selection approach (start with no variables and add one at a time), the backward selection approach (start with all variables and remove one at a time), or a sort of combination of the forward and backward approach.  

One question we must address is, what metric should we use in order to assess the results of comparing each of the different variable combinations. There are several options here, Mallow's $C_{p}$, AIC, BIC, and adjusted $R^2$. See Chapter 3 Section 3.2.2 of [An Introduction to Statistical Learning](http://faculty.marshall.usc.edu/gareth-james/ISL/) for more details. 

Rather than go through all of the details here, we will use the variable selection functions from the [olsrr package](https://cran.r-project.org/web/packages/olsrr/vignettes/intro.html). 

Let's start with an example based on simulated data:
```{r}
x1 <- rnorm(45,12,3.2)
x2 <- rnorm(45,3,0.2)
x3 <- rnorm(45,27,17)
y <-  0.1*x1 + 3*x3 - 35 + rnorm(45,sd=13)
sim_df <- data.frame(x1=x1,x2=x2,x3=x3,y=y)
```

Notice that we removed any dependence on x2. If we fit the model, here is the result:
```{r}
sim_lm_fit <- lm(y~.,data=sim_df)
summary(sim_lm_fit)
```

Since there are only three predictors, there are a total of eight possible variable combinations for a linear model so in this case it is not too "expensive" to fit all possible models and compare:
```{r}
(var_select_results<-ols_step_all_possible(sim_lm_fit))
```

Let's look at what all is contained in the output for this function. 
```{r}
names(var_select_results)
```
We see that the AIC values are also computed:
```{r}
var_select_results$aic
```
The model that minimizes AIC is regarded as best and we see that this is the model that includes on x1 and x3. 

**Question** Why do we expect this result? 

The plot function will produce a plot:
```{r}
plot(var_select_results)
```

Another similar function is
```{r}
(var_subset_result <- ols_step_best_subset(sim_lm_fit))
```

The plot function works on this as well:
```{r}
plot(var_subset_result)
```

There are also functions that implement forward and backward selection, for example:
```{r}
var_forward_select <- ols_step_forward_aic(sim_lm_fit)
```

```{r}
plot(var_forward_select)
```

### Example with Boston Housing Data

Let's try forward slection using AIC on the Boston housing data:
```{r}
Boston_lm_all <- lm(medv ~ . , data=Boston_df)
(Boston_selection <- ols_step_both_aic(Boston_lm_all))
```

```{r}
plot(Boston_selection)
```

```{r}
Boston_selection$predictors
```

```{r}
names(Boston_df)
```



**Note:** We note that variable selection can help to a certain extent in avoiding over-fitting but it is not the only point to consider regarding over-fitting because it still does not provide insight into the test MSE. Cross-validation (CV), we will discuss later, is another technique used to examine over-fitting related issues. CV is a ressampling method that provides a means to approximate test error. 

## Question 3)

How well does the model fit the data? 

The two most common numerical measures for model fit are residual standard error (RSE), which is an estimate of the standard deviation of $\epsilon$; and $R^2$, which describes the proportion of variation that is explained by the model. 

**Note:** Typical approaches to estimating the standard deviation of $\epsilon$ assume that this is a constant (independent of $x$). This assumption is known as **homoscedasticity**. Also, the RSE provides an absolute measure of *lack of fit*. A large RSE suggests a poor fit. $R^2$ is always between 0 and 1  and thus represents a proportion. An $R^2$ value close to 1 suggests that the model explains a large portion
of the variance in the response variable. 

The  output from the glance() function contains the values of RSE and $R^2$. For example, 

```{r}
glance(sim_lm_fit)
```
the frst column is $R^2$ and the column sigma is the RSE. Compare this with the output from 
```{r}
summary(sim_lm_fit)
```

**Note:** The residual versus fitted values plot is a good way to check for (the violation of) homoscedasticity. Consider the following simulated example:

```{r}
x <- rnorm(75,mean=2,sd=1.7)
heterogeneous_sd <- function(x) sapply(x,function(x) rnorm(1,mean=0,sd=5*abs(x)))
y <- 2*x + heterogeneous_sd(x)
df <- data.frame(x=x,y=y)
```

```{r}
df %>% ggplot(aes(x=x,y=y)) + geom_point() + 
  geom_smooth(method = "lm")
```

```{r}
lm_fit_heteroscedasticity <- lm(y~x,data=df)
augment(lm_fit_heteroscedasticity,data=df) %>%
  ggplot(aes(x=.fitted,y=.resid)) + geom_point() + 
  geom_smooth()
```

Here we can "observe" the violation of homoscedasticity.

## Question 4) 

Suppose we fit a linear model using some predictor/response pair of sampled data. There will be three main sources of error:

1) Error due to bias because of the likely simplifying assumption of linearity. 
2) The irreducible error that comes from estimating population parameters. This error is measured by confidence intervals. 
3) The irreducible error due to $\epsilon$. Error arising from $\epsilon$ in making predictions is measured by **prediction intervals**. These will always be wider than confidence intervals because they
incorporate both the error in the estimate for $f(X)$ and the uncertainty as to how much an individual point will differ from the population regression plane (the irreducible error). 


The predict() function can be used to return the prediction intervals for new values.  Let's look at an example. 


Let's go back to the simulated data
```{r}
x1 <- rnorm(45,12,3.2)
x2 <- rnorm(45,3,0.2)
x3 <- rnorm(45,27,17)
y <-  0.1*x1 + 15*x2 + 3*x3 - 35 + rnorm(45,sd=13)
sim_df <- data.frame(x1=x1,x2=x2,x3=x3,y=y)
```

We will fit a multiple linear regression model:
```{r}
sim_df_lm <- lm(y~.,data=sim_df)
```

Let's produce some more data:
```{r}
x1_new <- rnorm(10,12,3.2)
x2_new <- rnorm(10,3,0.2)
x3_new <- rnorm(10,27,17)
predict(sim_df_lm,newdata = data.frame(x1=x1_new,x2=x2_new,x3=x3_new),interval = "prediction")
```

Compare this with
```{r}
predict(sim_df_lm,newdata = data.frame(x1=x1_new,x2=x2_new,x3=x3_new),interval = "confidence")
```

It is also possible to use bootstrap to approximate prediction intervals but we will not need this very often so we will postpone any discussion of this. 

There are three more topics that we will address regarding regression modeling before then moving on the look at some methods for classification:

1) Estimating test error for regression models. This involves covering the idea of cross-validation. 
2) Dealing with nonlinearity.
3) Using regression for binary classification. 

These will be addressed in future notebooks. 

